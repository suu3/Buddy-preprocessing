{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "KoBert_15000",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMpbnh1rdUP6nkqIxVwe5B9"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_bwTQhc-EwG",
        "outputId": "1164cb34-6000-450e-a350-164e7096291c"
      },
      "source": [
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-p5tana8b\n",
            "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-p5tana8b\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD2qEE_4-KW8",
        "outputId": "cfba3e68-1182-47d6-cfe4-c127676a75eb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUiP_jiK2C2_",
        "outputId": "90cfef41-ad93-4d63-a3c2-9388eda801a8"
      },
      "source": [
        "!pip install -r /content/drive/'My Drive'/'Colab Notebooks'/buddy/requirements.txt"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kobert-transformers==0.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: kogpt2-transformers==0.3.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 2)) (0.3.0)\n",
            "Requirement already satisfied: transformers==3.0.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 4)) (1.9.0+cu102)\n",
            "Requirement already satisfied: tokenizers==0.8.1rc1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 5)) (0.8.1rc1)\n",
            "Requirement already satisfied: kss in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 6)) (2.5.1)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (1.1.4)\n",
            "Requirement already satisfied: flask_restful in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 8)) (0.3.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (21.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (0.1.96)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (0.0.45)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 4)) (3.7.4.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (2.0.1)\n",
            "Requirement already satisfied: aniso8601>=0.82 in /usr/local/lib/python3.7/dist-packages (from flask_restful->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 8)) (9.0.1)\n",
            "Requirement already satisfied: six>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from flask_restful->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 8)) (1.15.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from flask_restful->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 8)) (2018.9)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9ONaA7l2Hmh"
      },
      "source": [
        "import sys\n",
        "sys.path.append('drive/My Drive/Colab Notebooks/')\n",
        "sys.path.append('drive/My Drive/Colab Notebooks/buddy')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4X8hDOC2JOy"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from transformers import AdamW\n",
        "from torch.utils.data import dataloader\n",
        "from buddy.dataloader.wellness import WellnessTextClassificationDataset\n",
        "from buddy.model.kobert import KoBERTforSequenceClassfication"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOvLUSCQ2KxO",
        "outputId": "1260aa5e-59e2-462b-f017-21b16da3c828"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1P6OOtM2LfD"
      },
      "source": [
        "def train(device, epoch, model, optimizer, train_loader, save_step, save_ckpt_path, train_step = 0):\n",
        "    losses = []\n",
        "    train_start_index = train_step+1 if train_step != 0 else 0\n",
        "    total_train_step = len(train_loader)\n",
        "    model.train()\n",
        "\n",
        "    with tqdm(total= total_train_step, desc=f\"Train({epoch})\") as pbar:\n",
        "        pbar.update(train_step)\n",
        "        for i, data in enumerate(train_loader, train_start_index):\n",
        "          \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(**data)\n",
        "\n",
        "            loss = outputs[0]\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix_str(f\"Loss: {loss.item():.3f} ({np.mean(losses):.3f})\")\n",
        "\n",
        "            if i >= total_train_step or i % save_step == 0:\n",
        "                torch.save({\n",
        "                    'epoch': epoch,  # 현재 학습 epoch\n",
        "                    'model_state_dict': model.state_dict(),  # 모델 저장\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),  # 옵티마이저 저장\n",
        "                    'loss': loss.item(),  # Loss 저장\n",
        "                    'train_step': i,  # 현재 진행한 학습\n",
        "                    'total_train_step': len(train_loader)  # 현재 epoch에 학습 할 총 train step\n",
        "                }, save_ckpt_path)\n",
        "\n",
        "    return np.mean(losses)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meIpH3HW2NOO"
      },
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdLIHijo2N38",
        "outputId": "f8d98e13-d739-4129-a439-3a5786e2c9ce"
      },
      "source": [
        "root_path = \"/content/drive/MyDrive/Colab Notebooks/buddy\"\n",
        "data_path = f\"{root_path}/data2/wellness_dialog_for_text_classification_train.txt\"\n",
        "checkpoint_path =f\"{root_path}/checkpoint\"\n",
        "save_ckpt_path = f\"{checkpoint_path}/kobert-wellness-text-classification-15000-191.pth\"\n",
        "\n",
        "n_epoch = 50 #Num of Epoch\n",
        "batch_size = 4 #배치 사이즈 #Colab이 돌아가지 않아 4로 했으며, 증가시켜도 무방\n",
        "ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(ctx)\n",
        "save_step = 100 #학습 저장 주기\n",
        "learning_rate = 5e-6  #Learning Rate\n",
        "\n",
        "#WellnessTextClassificationDataset Data Loader\n",
        "dataset = WellnessTextClassificationDataset(file_path=data_path, device=device)\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = KoBERTforSequenceClassfication()\n",
        "model.to(device)\n",
        "\n",
        "#Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "      'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "\n",
        "pre_epoch, pre_loss, train_step = 0, 0, 0\n",
        "if os.path.isfile(save_ckpt_path):\n",
        "    checkpoint = torch.load(save_ckpt_path, map_location=device)\n",
        "    pre_epoch = checkpoint['epoch']\n",
        "    train_step =  checkpoint['train_step']\n",
        "    total_train_step =  checkpoint['total_train_step']\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    print(f\"load pretrain from: {save_ckpt_path}, epoch={pre_epoch}\")\n",
        "\n",
        "losses = []\n",
        "offset = pre_epoch\n",
        "for step in range(n_epoch):\n",
        "    epoch = step + offset\n",
        "    loss = train(device, epoch, model, optimizer, train_loader, save_step, save_ckpt_path, train_step)\n",
        "    losses.append(loss)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train(0): 100%|██████████| 3505/3505 [16:11<00:00,  3.61it/s, Loss: 3.869 (3.853)]\n",
            "Train(1): 100%|██████████| 3505/3505 [16:29<00:00,  3.54it/s, Loss: 2.142 (2.840)]\n",
            "Train(2): 100%|██████████| 3505/3505 [16:27<00:00,  3.55it/s, Loss: 1.173 (2.507)]\n",
            "Train(3): 100%|██████████| 3505/3505 [16:26<00:00,  3.55it/s, Loss: 1.727 (2.307)]\n",
            "Train(4): 100%|██████████| 3505/3505 [16:37<00:00,  3.52it/s, Loss: 4.621 (2.134)]\n",
            "Train(5): 100%|██████████| 3505/3505 [16:22<00:00,  3.57it/s, Loss: 2.499 (1.979)]\n",
            "Train(6): 100%|██████████| 3505/3505 [16:32<00:00,  3.53it/s, Loss: 1.704 (1.830)]\n",
            "Train(7): 100%|██████████| 3505/3505 [16:30<00:00,  3.54it/s, Loss: 4.152 (1.695)]\n",
            "Train(8): 100%|██████████| 3505/3505 [16:36<00:00,  3.52it/s, Loss: 1.392 (1.557)]\n",
            "Train(9): 100%|██████████| 3505/3505 [16:36<00:00,  3.52it/s, Loss: 0.450 (1.423)]\n",
            "Train(10): 100%|██████████| 3505/3505 [16:31<00:00,  3.53it/s, Loss: 1.203 (1.298)]\n",
            "Train(11): 100%|██████████| 3505/3505 [16:37<00:00,  3.51it/s, Loss: 1.364 (1.179)]\n",
            "Train(12): 100%|██████████| 3505/3505 [16:40<00:00,  3.50it/s, Loss: 1.236 (1.064)]\n",
            "Train(13): 100%|██████████| 3505/3505 [16:37<00:00,  3.51it/s, Loss: 1.370 (0.951)]\n",
            "Train(14): 100%|██████████| 3505/3505 [16:38<00:00,  3.51it/s, Loss: 0.069 (0.847)]\n",
            "Train(15): 100%|██████████| 3505/3505 [16:33<00:00,  3.53it/s, Loss: 0.029 (0.756)]\n",
            "Train(16): 100%|██████████| 3505/3505 [16:36<00:00,  3.52it/s, Loss: 0.037 (0.669)]\n",
            "Train(17): 100%|██████████| 3505/3505 [16:33<00:00,  3.53it/s, Loss: 1.845 (0.589)]\n",
            "Train(18): 100%|██████████| 3505/3505 [16:40<00:00,  3.50it/s, Loss: 0.087 (0.527)]\n",
            "Train(19): 100%|██████████| 3505/3505 [16:29<00:00,  3.54it/s, Loss: 0.043 (0.466)]\n",
            "Train(20): 100%|██████████| 3505/3505 [16:35<00:00,  3.52it/s, Loss: 0.101 (0.413)]\n",
            "Train(21): 100%|██████████| 3505/3505 [16:35<00:00,  3.52it/s, Loss: 0.280 (0.370)]\n",
            "Train(22): 100%|██████████| 3505/3505 [16:27<00:00,  3.55it/s, Loss: 1.533 (0.327)]\n",
            "Train(23): 100%|██████████| 3505/3505 [16:32<00:00,  3.53it/s, Loss: 0.100 (0.290)]\n",
            "Train(24): 100%|██████████| 3505/3505 [16:27<00:00,  3.55it/s, Loss: 0.207 (0.269)]\n",
            "Train(25): 100%|██████████| 3505/3505 [16:27<00:00,  3.55it/s, Loss: 0.154 (0.238)]\n",
            "Train(26): 100%|██████████| 3505/3505 [16:30<00:00,  3.54it/s, Loss: 1.128 (0.209)]\n",
            "Train(27): 100%|██████████| 3505/3505 [16:32<00:00,  3.53it/s, Loss: 0.018 (0.193)]\n",
            "Train(28): 100%|██████████| 3505/3505 [16:26<00:00,  3.55it/s, Loss: 0.027 (0.183)]\n",
            "Train(29): 100%|██████████| 3505/3505 [16:41<00:00,  3.50it/s, Loss: 0.015 (0.163)]\n",
            "Train(30): 100%|██████████| 3505/3505 [16:35<00:00,  3.52it/s, Loss: 0.008 (0.149)]\n",
            "Train(31): 100%|██████████| 3505/3505 [16:30<00:00,  3.54it/s, Loss: 0.005 (0.141)]\n",
            "Train(32): 100%|██████████| 3505/3505 [16:35<00:00,  3.52it/s, Loss: 0.002 (0.125)]\n",
            "Train(33): 100%|██████████| 3505/3505 [16:32<00:00,  3.53it/s, Loss: 0.003 (0.120)]\n",
            "Train(34): 100%|██████████| 3505/3505 [16:28<00:00,  3.55it/s, Loss: 0.026 (0.114)]\n",
            "Train(35): 100%|██████████| 3505/3505 [16:28<00:00,  3.55it/s, Loss: 0.065 (0.111)]\n",
            "Train(36): 100%|██████████| 3505/3505 [16:32<00:00,  3.53it/s, Loss: 0.002 (0.105)]\n",
            "Train(37): 100%|██████████| 3505/3505 [16:24<00:00,  3.56it/s, Loss: 0.992 (0.101)]\n",
            "Train(38): 100%|██████████| 3505/3505 [16:24<00:00,  3.56it/s, Loss: 0.017 (0.093)]\n",
            "Train(39): 100%|██████████| 3505/3505 [16:30<00:00,  3.54it/s, Loss: 0.138 (0.087)]\n",
            "Train(40): 100%|██████████| 3505/3505 [16:27<00:00,  3.55it/s, Loss: 0.008 (0.093)]\n",
            "Train(41): 100%|██████████| 3505/3505 [16:19<00:00,  3.58it/s, Loss: 0.088 (0.083)]\n",
            "Train(42): 100%|██████████| 3505/3505 [16:21<00:00,  3.57it/s, Loss: 0.042 (0.083)]\n",
            "Train(43): 100%|██████████| 3505/3505 [16:24<00:00,  3.56it/s, Loss: 0.024 (0.078)]\n",
            "Train(44): 100%|██████████| 3505/3505 [16:22<00:00,  3.57it/s, Loss: 0.002 (0.078)]\n",
            "Train(45): 100%|██████████| 3505/3505 [16:22<00:00,  3.57it/s, Loss: 0.014 (0.078)]\n",
            "Train(46): 100%|██████████| 3505/3505 [16:29<00:00,  3.54it/s, Loss: 0.013 (0.075)]\n",
            "Train(47): 100%|██████████| 3505/3505 [16:22<00:00,  3.57it/s, Loss: 0.024 (0.072)]\n",
            "Train(48): 100%|██████████| 3505/3505 [16:22<00:00,  3.57it/s, Loss: 0.021 (0.076)]\n",
            "Train(49): 100%|██████████| 3505/3505 [16:24<00:00,  3.56it/s, Loss: 0.003 (0.069)]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zu_s1DZYF-3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c40e8373-f00c-474d-affa-a115249bcc56"
      },
      "source": [
        "\n",
        "# data\n",
        "data = {\n",
        "    \"loss\": losses\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "display(df)\n",
        "\n",
        "# graph\n",
        "plt.figure(figsize=[12, 4])\n",
        "plt.plot(losses, label=\"loss\")\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.852808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.840363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.507285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.306658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.133550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.978812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.830096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.694700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.556910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.423048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.298073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1.179406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1.063779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.950517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.847256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.755506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.668636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.589407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.527319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.465819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.412611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.370155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.326865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.290154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.268873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.237880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.209271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.192838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.183048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.162711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.148679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.141295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.125291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.120459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.114463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.110682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.105290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.101456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.092847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.086942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.092917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.083413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.083474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.078346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.078374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.078053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.074701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.072004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.076107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.068800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        loss\n",
              "0   3.852808\n",
              "1   2.840363\n",
              "2   2.507285\n",
              "3   2.306658\n",
              "4   2.133550\n",
              "5   1.978812\n",
              "6   1.830096\n",
              "7   1.694700\n",
              "8   1.556910\n",
              "9   1.423048\n",
              "10  1.298073\n",
              "11  1.179406\n",
              "12  1.063779\n",
              "13  0.950517\n",
              "14  0.847256\n",
              "15  0.755506\n",
              "16  0.668636\n",
              "17  0.589407\n",
              "18  0.527319\n",
              "19  0.465819\n",
              "20  0.412611\n",
              "21  0.370155\n",
              "22  0.326865\n",
              "23  0.290154\n",
              "24  0.268873\n",
              "25  0.237880\n",
              "26  0.209271\n",
              "27  0.192838\n",
              "28  0.183048\n",
              "29  0.162711\n",
              "30  0.148679\n",
              "31  0.141295\n",
              "32  0.125291\n",
              "33  0.120459\n",
              "34  0.114463\n",
              "35  0.110682\n",
              "36  0.105290\n",
              "37  0.101456\n",
              "38  0.092847\n",
              "39  0.086942\n",
              "40  0.092917\n",
              "41  0.083413\n",
              "42  0.083474\n",
              "43  0.078346\n",
              "44  0.078374\n",
              "45  0.078053\n",
              "46  0.074701\n",
              "47  0.072004\n",
              "48  0.076107\n",
              "49  0.068800"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xKaN3xkMUIV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "6f6aeb73-683b-4314-f0f9-66fb19359cec"
      },
      "source": [
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "from model.kobert import KoBERTforSequenceClassfication, kobert_input\n",
        "from kobert_transformers import get_tokenizer\n",
        "\n",
        "def load_wellness_answer():\n",
        "  root_path = \"/content/drive/MyDrive/Colab Notebooks/buddy\"\n",
        "  category_path = f\"{root_path}/data2/wellness_dialog_category.txt\"\n",
        "  answer_path = f\"{root_path}/data2/wellness_dialog_answer.txt\"\n",
        "\n",
        "  c_f = open(category_path,'r')\n",
        "  a_f = open(answer_path,'r')\n",
        "\n",
        "  category_lines = c_f.readlines()\n",
        "  answer_lines = a_f.readlines()\n",
        "\n",
        "  category = {}\n",
        "  answer = {}\n",
        "  for line_num, line_data in enumerate(category_lines):\n",
        "    data = line_data.split('    ')\n",
        "    category[data[1][:-1]]=data[0]\n",
        "  \n",
        "  for line_num, line_data in enumerate(answer_lines):\n",
        "    data = line_data.split('    ')\n",
        "    keys = answer.keys()\n",
        "    if(data[0] in keys):\n",
        "      answer[data[0]] += [data[1][:-1]]\n",
        "    else:\n",
        "      answer[data[0]] = [data[1][:-1]]\n",
        "\n",
        "  return category, answer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  root_path = \"/content/drive/MyDrive/Colab Notebooks/buddy\"\n",
        "  checkpoint_path =f\"{root_path}/checkpoint\"\n",
        "  save_ckpt_path = f\"{checkpoint_path}/kobert-wellness-text-classification-15000-191.pth\"\n",
        "\n",
        "  #답변과 카테고리 불러오기\n",
        "  category, answer = load_wellness_answer()\n",
        "\n",
        "  ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  device = torch.device(ctx)\n",
        "\n",
        "  # 저장한 Checkpoint 불러오기\n",
        "  checkpoint = torch.load(save_ckpt_path, map_location=device)\n",
        "\n",
        "  model = KoBERTforSequenceClassfication()\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "  model.to(ctx)\n",
        "  model.eval()\n",
        "\n",
        "  tokenizer = get_tokenizer()\n",
        "\n",
        "  while 1:\n",
        "    sent = input('\\nQuestion: ') # '요즘 기분이 우울한 느낌이에요'\n",
        "    data = kobert_input(tokenizer, sent, device, 512)\n",
        "\n",
        "    if '종료' in sent:\n",
        "      break\n",
        "\n",
        "    output = model(**data)\n",
        "\n",
        "    logit = output[0]\n",
        "    softmax_logit = torch.softmax(logit,dim=-1)\n",
        "    softmax_logit = softmax_logit.squeeze()\n",
        "\n",
        "    max_index = torch.argmax(softmax_logit).item()\n",
        "    max_index_value = softmax_logit[torch.argmax(softmax_logit)].item()\n",
        "\n",
        "    answer_list = answer[category[str(max_index)]]\n",
        "    answer_len= len(answer_list)-1\n",
        "    answer_index = random.randint(0,answer_len)\n",
        "    print(f'Answer: {answer_list[answer_index]}, index: {max_index}, softmax_value: {max_index_value}')\n",
        "    print('-'*50)\n",
        "'''\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-a9490093d25d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nQuestion: '\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# '요즘 기분이 우울한 느낌이에요'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkobert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW0_62qtJD3q"
      },
      "source": [
        "평가함수\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6l63ZOTJDs5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7cb7b08-2190-4fa2-b8ca-b2f137a7269e"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "  AdamW,\n",
        "  ElectraConfig,\n",
        "  ElectraTokenizer\n",
        ")\n",
        "from torch.utils.data import dataloader\n",
        "from dataloader.wellness import WellnessTextClassificationDataset\n",
        "#from model.koelectra import koElectraForSequenceClassification\n",
        "from model.kobert import KoBERTforSequenceClassfication\n",
        "from kobert_transformers import get_tokenizer\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "MODEL_CLASSES ={\n",
        "  #\"koelectra\": (ElectraConfig, koElectraForSequenceClassification, ElectraTokenizer),\n",
        "  \"kobert\": (KoBERTforSequenceClassfication)\n",
        "}\n",
        "CHECK_POINT ={\n",
        "  #\"koelectra\": \"/content/drive/MyDrive/Colab Notebooks/buddy/checkpoint/koelectra-wellnesee-text-classification.pth\",\n",
        "  \"kobert\": \"/content/drive/MyDrive/Colab Notebooks/buddy/checkpoint/kobert-wellness-text-classification-15000-191.pth\"\n",
        "}\n",
        "\n",
        "def get_model_and_tokenizer(model_name, device):\n",
        "  save_ckpt_path = CHECK_POINT[model_name]\n",
        "\n",
        "#  if model_name== \"koelectra\":\n",
        "#    model_name_or_path = \"monologg/koelectra-base-discriminator\"\n",
        "\n",
        "#    tokenizer = ElectraTokenizer.from_pretrained(model_name_or_path)\n",
        "#    electra_config = ElectraConfig.from_pretrained(model_name_or_path)\n",
        "#    model = koElectraForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path,\n",
        "#                                                               config=electra_config,\n",
        "#                                                               num_labels=359)\n",
        "  if model_name =='kobert':\n",
        "    tokenizer = get_tokenizer()\n",
        "    model = KoBERTforSequenceClassfication()\n",
        "\n",
        "  if os.path.isfile(save_ckpt_path):\n",
        "      checkpoint = torch.load(save_ckpt_path, map_location=device)\n",
        "      pre_epoch = checkpoint['epoch']\n",
        "      # pre_loss = checkpoint['loss']\n",
        "      model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "      print(f\"load pretrain from: {save_ckpt_path}, epoch={pre_epoch}\")\n",
        "\n",
        "  return model, tokenizer\n",
        "\n",
        "def get_model_input(data):\n",
        "  if model_name =='kobert':\n",
        "    return data\n",
        "#  elif model_name== \"koelectra\":\n",
        "#    return {'input_ids': data['input_ids'],\n",
        "#              'attention_mask': data['attention_mask'],\n",
        "#              'labels': data['labels']\n",
        "#              }\n",
        "\n",
        "def evaluate(model_name, device, batch_size, data_path):\n",
        "\n",
        "  model, tokenizer = get_model_and_tokenizer(model_name, device)\n",
        "  model.to(device)\n",
        "\n",
        "  # WellnessTextClassificationDataset 데이터 로더\n",
        "  eval_dataset = WellnessTextClassificationDataset(file_path=data_path,device=device, tokenizer=tokenizer)\n",
        "  eval_dataloader = torch.utils.data.DataLoader(eval_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  logger.info(\"***** Running evaluation on %s dataset *****\")\n",
        "  logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "  logger.info(\"  Batch size = %d\", batch_size)\n",
        "\n",
        "  loss = 0\n",
        "  acc = 0\n",
        "\n",
        "\n",
        "  # model.eval()\n",
        "  for data in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "    with torch.no_grad():\n",
        "      inputs = get_model_input(data)\n",
        "      outputs = model(**inputs)\n",
        "      loss += outputs[0]\n",
        "      logit = outputs[1]\n",
        "      acc += (logit.argmax(1)==inputs['labels']).sum().item()\n",
        "\n",
        "  return loss / len(eval_dataset), acc / len(eval_dataset)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  #root_path = \"/content/drive/MyDrive/Colab Notebooks/buddy/data\"\n",
        "  root_path = \"/content/drive/MyDrive/Colab Notebooks/buddy\"\n",
        "  data_path = f\"{root_path}/data2/wellness_dialog_for_text_classification_test.txt\"\n",
        "  checkpoint_path = f\"{root_path}/checkpoint\"\n",
        "  save_ckpt_path = f\"{checkpoint_path}/kobert-wellness-text-classification-15000-191.pth\"\n",
        "  #model_name_or_path = \"monologg/koelectra-base-discriminator\"\n",
        "\n",
        "  n_epoch = 50  # Num of Epoch\n",
        "  batch_size = 16  # 배치 사이즈\n",
        "  ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  device = torch.device(ctx)\n",
        "  model_names=[\"kobert\"]\n",
        "  for model_name in model_names:\n",
        "    eval_loss, eval_acc = evaluate(model_name, device, batch_size, data_path)\n",
        "    print(f'\\tLoss: {eval_loss:.4f}(valid)\\t|\\tAcc: {eval_acc * 100:.1f}%(valid)')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load pretrain from: /content/drive/MyDrive/Colab Notebooks/buddy/checkpoint/kobert-wellness-text-classification-15000-191.pth, epoch=49\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 86/86 [00:24<00:00,  3.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tLoss: 0.1974(valid)\t|\tAcc: 51.4%(valid)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73KXoMIagVS8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}