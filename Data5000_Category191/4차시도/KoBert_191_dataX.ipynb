{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "KoBert_191_dataX",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNzNlYHFIrVszg7qAzxQ6Ot",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suu3/Capstone/blob/main/KoBert_191_dataX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_bwTQhc-EwG",
        "outputId": "e6ea7c4b-bb67-459b-9d5d-e437e342ce33"
      },
      "source": [
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-hol3jedx\n",
            "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-hol3jedx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD2qEE_4-KW8",
        "outputId": "892f2eac-ac79-48af-d6f1-c62441a5a954"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUiP_jiK2C2_",
        "outputId": "fd87f0bc-ddee-45bb-ae20-33f2b450c6a1"
      },
      "source": [
        "!pip install -r /content/drive/'My Drive'/'Colab Notebooks'/buddy/requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kobert-transformers==0.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: kogpt2-transformers==0.3.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 2)) (0.3.0)\n",
            "Requirement already satisfied: transformers==3.0.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 4)) (1.9.0+cu102)\n",
            "Requirement already satisfied: tokenizers==0.8.1rc1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 5)) (0.8.1rc1)\n",
            "Requirement already satisfied: kss in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 6)) (2.5.1)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (1.1.4)\n",
            "Requirement already satisfied: flask_restful in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 8)) (0.3.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (0.0.45)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (0.1.96)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 4)) (3.7.4.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (2.0.1)\n",
            "Requirement already satisfied: aniso8601>=0.82 in /usr/local/lib/python3.7/dist-packages (from flask_restful->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 8)) (9.0.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from flask_restful->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 8)) (2018.9)\n",
            "Requirement already satisfied: six>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from flask_restful->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 8)) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9ONaA7l2Hmh"
      },
      "source": [
        "import sys\n",
        "sys.path.append('drive/My Drive/Colab Notebooks/')\n",
        "sys.path.append('drive/My Drive/Colab Notebooks/buddy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4X8hDOC2JOy"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from transformers import AdamW\n",
        "from torch.utils.data import dataloader\n",
        "from buddy.dataloader.wellness import WellnessTextClassificationDataset\n",
        "from buddy.model.kobert import KoBERTforSequenceClassfication"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOvLUSCQ2KxO",
        "outputId": "05f4b256-ca2a-45eb-a485-b397b9350532"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1P6OOtM2LfD"
      },
      "source": [
        "def train(device, epoch, model, optimizer, train_loader, save_step, save_ckpt_path, train_step = 0):\n",
        "    losses = []\n",
        "    train_start_index = train_step+1 if train_step != 0 else 0\n",
        "    total_train_step = len(train_loader)\n",
        "    model.train()\n",
        "\n",
        "    with tqdm(total= total_train_step, desc=f\"Train({epoch})\") as pbar:\n",
        "        pbar.update(train_step)\n",
        "        for i, data in enumerate(train_loader, train_start_index):\n",
        "          \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(**data)\n",
        "\n",
        "            loss = outputs[0]\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix_str(f\"Loss: {loss.item():.3f} ({np.mean(losses):.3f})\")\n",
        "\n",
        "            if i >= total_train_step or i % save_step == 0:\n",
        "                torch.save({\n",
        "                    'epoch': epoch,  # 현재 학습 epoch\n",
        "                    'model_state_dict': model.state_dict(),  # 모델 저장\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),  # 옵티마이저 저장\n",
        "                    'loss': loss.item(),  # Loss 저장\n",
        "                    'train_step': i,  # 현재 진행한 학습\n",
        "                    'total_train_step': len(train_loader)  # 현재 epoch에 학습 할 총 train step\n",
        "                }, save_ckpt_path)\n",
        "\n",
        "    return np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meIpH3HW2NOO"
      },
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdLIHijo2N38",
        "outputId": "8755c4a5-b335-43d6-8f42-945dd670410f"
      },
      "source": [
        "root_path = \"/content/drive/MyDrive/Colab Notebooks/buddy\"\n",
        "data_path = f\"{root_path}/data2/wellness_dialog_for_text_classification_train.txt\"\n",
        "checkpoint_path =f\"{root_path}/checkpoint\"\n",
        "save_ckpt_path = f\"{checkpoint_path}/kobert-wellness-text-classification-dataX-191.pth\"\n",
        "\n",
        "n_epoch = 50 #Num of Epoch\n",
        "batch_size = 4 #배치 사이즈 #Colab이 돌아가지 않아 4로 했으며, 증가시켜도 무방\n",
        "ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(ctx)\n",
        "save_step = 100 #학습 저장 주기\n",
        "learning_rate = 5e-6  #Learning Rate\n",
        "\n",
        "#WellnessTextClassificationDataset Data Loader\n",
        "dataset = WellnessTextClassificationDataset(file_path=data_path, device=device)\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = KoBERTforSequenceClassfication()\n",
        "model.to(device)\n",
        "\n",
        "#Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "      'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "\n",
        "pre_epoch, pre_loss, train_step = 0, 0, 0\n",
        "if os.path.isfile(save_ckpt_path):\n",
        "    checkpoint = torch.load(save_ckpt_path, map_location=device)\n",
        "    pre_epoch = checkpoint['epoch']\n",
        "    train_step =  checkpoint['train_step']\n",
        "    total_train_step =  checkpoint['total_train_step']\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    print(f\"load pretrain from: {save_ckpt_path}, epoch={pre_epoch}\")\n",
        "\n",
        "losses = []\n",
        "offset = pre_epoch\n",
        "for step in range(n_epoch):\n",
        "    epoch = step + offset\n",
        "    loss = train(device, epoch, model, optimizer, train_loader, save_step, save_ckpt_path, train_step)\n",
        "    losses.append(loss)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train(0): 100%|██████████| 1234/1234 [05:49<00:00,  3.53it/s, Loss: 5.951 (4.790)]\n",
            "Train(1): 100%|██████████| 1234/1234 [05:47<00:00,  3.55it/s, Loss: 1.976 (4.284)]\n",
            "Train(2): 100%|██████████| 1234/1234 [05:45<00:00,  3.57it/s, Loss: 5.585 (4.091)]\n",
            "Train(3): 100%|██████████| 1234/1234 [05:53<00:00,  3.49it/s, Loss: 0.051 (3.830)]\n",
            "Train(4): 100%|██████████| 1234/1234 [05:57<00:00,  3.46it/s, Loss: 0.510 (3.541)]\n",
            "Train(5): 100%|██████████| 1234/1234 [05:55<00:00,  3.47it/s, Loss: 4.698 (3.257)]\n",
            "Train(6): 100%|██████████| 1234/1234 [05:54<00:00,  3.48it/s, Loss: 1.122 (3.002)]\n",
            "Train(7): 100%|██████████| 1234/1234 [05:57<00:00,  3.45it/s, Loss: 3.274 (2.759)]\n",
            "Train(8): 100%|██████████| 1234/1234 [05:59<00:00,  3.44it/s, Loss: 4.978 (2.544)]\n",
            "Train(9): 100%|██████████| 1234/1234 [05:54<00:00,  3.48it/s, Loss: 6.885 (2.327)]\n",
            "Train(10): 100%|██████████| 1234/1234 [05:53<00:00,  3.49it/s, Loss: 2.322 (2.125)]\n",
            "Train(11): 100%|██████████| 1234/1234 [05:52<00:00,  3.50it/s, Loss: 1.800 (1.946)]\n",
            "Train(12): 100%|██████████| 1234/1234 [05:48<00:00,  3.54it/s, Loss: 0.772 (1.772)]\n",
            "Train(13): 100%|██████████| 1234/1234 [05:57<00:00,  3.45it/s, Loss: 0.024 (1.606)]\n",
            "Train(14): 100%|██████████| 1234/1234 [05:57<00:00,  3.45it/s, Loss: 0.194 (1.452)]\n",
            "Train(15): 100%|██████████| 1234/1234 [06:01<00:00,  3.41it/s, Loss: 0.036 (1.307)]\n",
            "Train(16): 100%|██████████| 1234/1234 [05:59<00:00,  3.44it/s, Loss: 0.166 (1.169)]\n",
            "Train(17): 100%|██████████| 1234/1234 [05:57<00:00,  3.45it/s, Loss: 0.067 (1.035)]\n",
            "Train(18): 100%|██████████| 1234/1234 [05:57<00:00,  3.45it/s, Loss: 0.037 (0.917)]\n",
            "Train(19): 100%|██████████| 1234/1234 [05:59<00:00,  3.43it/s, Loss: 0.228 (0.806)]\n",
            "Train(20): 100%|██████████| 1234/1234 [06:00<00:00,  3.43it/s, Loss: 0.176 (0.714)]\n",
            "Train(21): 100%|██████████| 1234/1234 [05:56<00:00,  3.46it/s, Loss: 0.223 (0.611)]\n",
            "Train(22): 100%|██████████| 1234/1234 [05:58<00:00,  3.44it/s, Loss: 0.500 (0.537)]\n",
            "Train(23): 100%|██████████| 1234/1234 [05:51<00:00,  3.51it/s, Loss: 1.876 (0.464)]\n",
            "Train(24): 100%|██████████| 1234/1234 [05:50<00:00,  3.52it/s, Loss: 0.032 (0.391)]\n",
            "Train(25): 100%|██████████| 1234/1234 [05:56<00:00,  3.47it/s, Loss: 0.334 (0.343)]\n",
            "Train(26): 100%|██████████| 1234/1234 [05:53<00:00,  3.49it/s, Loss: 0.362 (0.287)]\n",
            "Train(27): 100%|██████████| 1234/1234 [05:47<00:00,  3.55it/s, Loss: 0.003 (0.235)]\n",
            "Train(28): 100%|██████████| 1234/1234 [05:52<00:00,  3.50it/s, Loss: 0.011 (0.195)]\n",
            "Train(29): 100%|██████████| 1234/1234 [05:49<00:00,  3.53it/s, Loss: 0.005 (0.170)]\n",
            "Train(30): 100%|██████████| 1234/1234 [05:49<00:00,  3.53it/s, Loss: 0.009 (0.142)]\n",
            "Train(31): 100%|██████████| 1234/1234 [05:52<00:00,  3.50it/s, Loss: 0.009 (0.127)]\n",
            "Train(32): 100%|██████████| 1234/1234 [05:55<00:00,  3.47it/s, Loss: 0.009 (0.097)]\n",
            "Train(33): 100%|██████████| 1234/1234 [05:49<00:00,  3.53it/s, Loss: 0.834 (0.079)]\n",
            "Train(34): 100%|██████████| 1234/1234 [05:54<00:00,  3.48it/s, Loss: 0.000 (0.079)]\n",
            "Train(35): 100%|██████████| 1234/1234 [05:48<00:00,  3.54it/s, Loss: 0.013 (0.060)]\n",
            "Train(36): 100%|██████████| 1234/1234 [05:53<00:00,  3.49it/s, Loss: 1.517 (0.054)]\n",
            "Train(37): 100%|██████████| 1234/1234 [05:55<00:00,  3.48it/s, Loss: 0.000 (0.053)]\n",
            "Train(38): 100%|██████████| 1234/1234 [05:55<00:00,  3.47it/s, Loss: 0.040 (0.042)]\n",
            "Train(39): 100%|██████████| 1234/1234 [05:51<00:00,  3.52it/s, Loss: 0.037 (0.034)]\n",
            "Train(40): 100%|██████████| 1234/1234 [05:51<00:00,  3.51it/s, Loss: 0.013 (0.040)]\n",
            "Train(41): 100%|██████████| 1234/1234 [05:53<00:00,  3.49it/s, Loss: 0.002 (0.019)]\n",
            "Train(42): 100%|██████████| 1234/1234 [05:55<00:00,  3.47it/s, Loss: 0.000 (0.028)]\n",
            "Train(43): 100%|██████████| 1234/1234 [05:56<00:00,  3.46it/s, Loss: 0.043 (0.039)]\n",
            "Train(44): 100%|██████████| 1234/1234 [05:52<00:00,  3.50it/s, Loss: 0.000 (0.036)]\n",
            "Train(45): 100%|██████████| 1234/1234 [05:54<00:00,  3.48it/s, Loss: 0.025 (0.021)]\n",
            "Train(46): 100%|██████████| 1234/1234 [05:55<00:00,  3.47it/s, Loss: 0.007 (0.017)]\n",
            "Train(47): 100%|██████████| 1234/1234 [05:52<00:00,  3.50it/s, Loss: 0.106 (0.019)]\n",
            "Train(48): 100%|██████████| 1234/1234 [05:56<00:00,  3.46it/s, Loss: 0.012 (0.024)]\n",
            "Train(49): 100%|██████████| 1234/1234 [05:50<00:00,  3.52it/s, Loss: 0.021 (0.013)]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zu_s1DZYF-3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b25ac79-a30b-4dad-fec7-422fd5e13b02"
      },
      "source": [
        "\n",
        "# data\n",
        "data = {\n",
        "    \"loss\": losses\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "display(df)\n",
        "\n",
        "# graph\n",
        "plt.figure(figsize=[12, 4])\n",
        "plt.plot(losses, label=\"loss\")\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.789775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.284114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.090914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.830083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.540684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3.257124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3.002123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2.759218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2.543966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2.326502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2.125468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1.946002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1.771633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1.606250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1.452419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1.306705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1.168597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1.035143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.916623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.805605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.713900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.610599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.536814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.463622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.390689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.343109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.287256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.234773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.194586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.169540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.142251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.126796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.097094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.079443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.078742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.059946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.053714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.052726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.041996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.034109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.039947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.018682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.028202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.038932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.035527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.021479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.017170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.019373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.024163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.013212</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        loss\n",
              "0   4.789775\n",
              "1   4.284114\n",
              "2   4.090914\n",
              "3   3.830083\n",
              "4   3.540684\n",
              "5   3.257124\n",
              "6   3.002123\n",
              "7   2.759218\n",
              "8   2.543966\n",
              "9   2.326502\n",
              "10  2.125468\n",
              "11  1.946002\n",
              "12  1.771633\n",
              "13  1.606250\n",
              "14  1.452419\n",
              "15  1.306705\n",
              "16  1.168597\n",
              "17  1.035143\n",
              "18  0.916623\n",
              "19  0.805605\n",
              "20  0.713900\n",
              "21  0.610599\n",
              "22  0.536814\n",
              "23  0.463622\n",
              "24  0.390689\n",
              "25  0.343109\n",
              "26  0.287256\n",
              "27  0.234773\n",
              "28  0.194586\n",
              "29  0.169540\n",
              "30  0.142251\n",
              "31  0.126796\n",
              "32  0.097094\n",
              "33  0.079443\n",
              "34  0.078742\n",
              "35  0.059946\n",
              "36  0.053714\n",
              "37  0.052726\n",
              "38  0.041996\n",
              "39  0.034109\n",
              "40  0.039947\n",
              "41  0.018682\n",
              "42  0.028202\n",
              "43  0.038932\n",
              "44  0.035527\n",
              "45  0.021479\n",
              "46  0.017170\n",
              "47  0.019373\n",
              "48  0.024163\n",
              "49  0.013212"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xKaN3xkMUIV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "0bc5b2e1-377b-482f-e1a3-650184d4aa4e"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "from model.kobert import KoBERTforSequenceClassfication, kobert_input\n",
        "from kobert_transformers import get_tokenizer\n",
        "\n",
        "def load_wellness_answer():\n",
        "  root_path = \"/content/drive/MyDrive/Colab Notebooks/buddy\"\n",
        "  category_path = f\"{root_path}/data2/wellness_dialog_category.txt\"\n",
        "  answer_path = f\"{root_path}/data2/wellness_dialog_answer.txt\"\n",
        "\n",
        "  c_f = open(category_path,'r')\n",
        "  a_f = open(answer_path,'r')\n",
        "\n",
        "  category_lines = c_f.readlines()\n",
        "  answer_lines = a_f.readlines()\n",
        "\n",
        "  category = {}\n",
        "  answer = {}\n",
        "  for line_num, line_data in enumerate(category_lines):\n",
        "    data = line_data.split('    ')\n",
        "    category[data[1][:-1]]=data[0]\n",
        "  \n",
        "  for line_num, line_data in enumerate(answer_lines):\n",
        "    data = line_data.split('    ')\n",
        "    keys = answer.keys()\n",
        "    if(data[0] in keys):\n",
        "      answer[data[0]] += [data[1][:-1]]\n",
        "    else:\n",
        "      answer[data[0]] = [data[1][:-1]]\n",
        "\n",
        "  return category, answer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  root_path = \"/content/drive/MyDrive/Colab Notebooks/buddy\"\n",
        "  checkpoint_path =f\"{root_path}/checkpoint\"\n",
        "  save_ckpt_path = f\"{checkpoint_path}/kobert-wellness-text-classification-dataX-191.pth\"\n",
        "\n",
        "  #답변과 카테고리 불러오기\n",
        "  category, answer = load_wellness_answer()\n",
        "\n",
        "  ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  device = torch.device(ctx)\n",
        "\n",
        "  # 저장한 Checkpoint 불러오기\n",
        "  checkpoint = torch.load(save_ckpt_path, map_location=device)\n",
        "\n",
        "  model = KoBERTforSequenceClassfication()\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "  model.to(ctx)\n",
        "  model.eval()\n",
        "\n",
        "  tokenizer = get_tokenizer()\n",
        "\n",
        "  while 1:\n",
        "    sent = input('\\nQuestion: ') # '요즘 기분이 우울한 느낌이에요'\n",
        "    data = kobert_input(tokenizer, sent, device, 512)\n",
        "\n",
        "    if '종료' in sent:\n",
        "      break\n",
        "\n",
        "    output = model(**data)\n",
        "\n",
        "    logit = output[0]\n",
        "    softmax_logit = torch.softmax(logit,dim=-1)\n",
        "    softmax_logit = softmax_logit.squeeze()\n",
        "\n",
        "    max_index = torch.argmax(softmax_logit).item()\n",
        "    max_index_value = softmax_logit[torch.argmax(softmax_logit)].item()\n",
        "\n",
        "    answer_list = answer[category[str(max_index)]]\n",
        "    answer_len= len(answer_list)-1\n",
        "    answer_index = random.randint(0,answer_len)\n",
        "    print(f'Answer: {answer_list[answer_index]}, index: {max_index}, softmax_value: {max_index_value}')\n",
        "    print('-'*50)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-ef4c46252a1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nQuestion: '\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# '요즘 기분이 우울한 느낌이에요'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkobert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW0_62qtJD3q"
      },
      "source": [
        "평가함수\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6l63ZOTJDs5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20e7de53-b30c-4cd8-8042-0fd966bb2f13"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "  AdamW,\n",
        "  ElectraConfig,\n",
        "  ElectraTokenizer\n",
        ")\n",
        "from torch.utils.data import dataloader\n",
        "from dataloader.wellness import WellnessTextClassificationDataset\n",
        "#from model.koelectra import koElectraForSequenceClassification\n",
        "from model.kobert import KoBERTforSequenceClassfication\n",
        "from kobert_transformers import get_tokenizer\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "MODEL_CLASSES ={\n",
        "  #\"koelectra\": (ElectraConfig, koElectraForSequenceClassification, ElectraTokenizer),\n",
        "  \"kobert\": (KoBERTforSequenceClassfication)\n",
        "}\n",
        "CHECK_POINT ={\n",
        "  #\"koelectra\": \"/content/drive/MyDrive/Colab Notebooks/buddy/checkpoint/koelectra-wellnesee-text-classification.pth\",\n",
        "  \"kobert\": \"/content/drive/MyDrive/Colab Notebooks/buddy/checkpoint/kobert-wellness-text-classification-dataX-191.pth\"\n",
        "}\n",
        "\n",
        "def get_model_and_tokenizer(model_name, device):\n",
        "  save_ckpt_path = CHECK_POINT[model_name]\n",
        "\n",
        "#  if model_name== \"koelectra\":\n",
        "#    model_name_or_path = \"monologg/koelectra-base-discriminator\"\n",
        "\n",
        "#    tokenizer = ElectraTokenizer.from_pretrained(model_name_or_path)\n",
        "#    electra_config = ElectraConfig.from_pretrained(model_name_or_path)\n",
        "#    model = koElectraForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path,\n",
        "#                                                               config=electra_config,\n",
        "#                                                               num_labels=359)\n",
        "  if model_name =='kobert':\n",
        "    tokenizer = get_tokenizer()\n",
        "    model = KoBERTforSequenceClassfication()\n",
        "\n",
        "  if os.path.isfile(save_ckpt_path):\n",
        "      checkpoint = torch.load(save_ckpt_path, map_location=device)\n",
        "      pre_epoch = checkpoint['epoch']\n",
        "      # pre_loss = checkpoint['loss']\n",
        "      model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "      print(f\"load pretrain from: {save_ckpt_path}, epoch={pre_epoch}\")\n",
        "\n",
        "  return model, tokenizer\n",
        "\n",
        "def get_model_input(data):\n",
        "  if model_name =='kobert':\n",
        "    return data\n",
        "#  elif model_name== \"koelectra\":\n",
        "#    return {'input_ids': data['input_ids'],\n",
        "#              'attention_mask': data['attention_mask'],\n",
        "#              'labels': data['labels']\n",
        "#              }\n",
        "\n",
        "def evaluate(model_name, device, batch_size, data_path):\n",
        "\n",
        "  model, tokenizer = get_model_and_tokenizer(model_name, device)\n",
        "  model.to(device)\n",
        "\n",
        "  # WellnessTextClassificationDataset 데이터 로더\n",
        "  eval_dataset = WellnessTextClassificationDataset(file_path=data_path,device=device, tokenizer=tokenizer)\n",
        "  eval_dataloader = torch.utils.data.DataLoader(eval_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  logger.info(\"***** Running evaluation on %s dataset *****\")\n",
        "  logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "  logger.info(\"  Batch size = %d\", batch_size)\n",
        "\n",
        "  loss = 0\n",
        "  acc = 0\n",
        "\n",
        "\n",
        "  # model.eval()\n",
        "  for data in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "    with torch.no_grad():\n",
        "      inputs = get_model_input(data)\n",
        "      outputs = model(**inputs)\n",
        "      loss += outputs[0]\n",
        "      logit = outputs[1]\n",
        "      acc += (logit.argmax(1)==inputs['labels']).sum().item()\n",
        "\n",
        "  return loss / len(eval_dataset), acc / len(eval_dataset)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  #root_path = \"/content/drive/MyDrive/Colab Notebooks/buddy/data\"\n",
        "  root_path = \"/content/drive/MyDrive/Colab Notebooks/buddy\"\n",
        "  data_path = f\"{root_path}/data/wellness_dialog_for_text_classification_test.txt\"\n",
        "  checkpoint_path = f\"{root_path}/checkpoint\"\n",
        "  save_ckpt_path = f\"{checkpoint_path}/kobert-wellness-text-classification-dataX-191.pth\"\n",
        "  #model_name_or_path = \"monologg/koelectra-base-discriminator\"\n",
        "\n",
        "  n_epoch = 50  # Num of Epoch\n",
        "  batch_size = 16  # 배치 사이즈\n",
        "  ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  device = torch.device(ctx)\n",
        "  model_names=[\"kobert\"]\n",
        "  for model_name in model_names:\n",
        "    eval_loss, eval_acc = evaluate(model_name, device, batch_size, data_path)\n",
        "    print(f'\\tLoss: {eval_loss:.4f}(valid)\\t|\\tAcc: {eval_acc * 100:.1f}%(valid)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load pretrain from: /content/drive/MyDrive/Colab Notebooks/buddy/checkpoint/kobert-wellness-text-classification-dataX-191.pth, epoch=49\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 33/33 [00:09<00:00,  3.52it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tLoss: 0.8974(valid)\t|\tAcc: 0.4%(valid)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}