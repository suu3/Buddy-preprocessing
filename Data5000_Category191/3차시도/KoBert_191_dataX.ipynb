{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "KoBert_191_dataX",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM5/oi6H89LFp5G4nhBkwQI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suu3/Capstone/blob/main/KoBert_191_dataX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_bwTQhc-EwG",
        "outputId": "8c471fd7-f08e-4870-f8d1-c7f49f96b267"
      },
      "source": [
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-fct0tj_8\n",
            "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-fct0tj_8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD2qEE_4-KW8",
        "outputId": "8a35c8f6-d298-42c4-c43f-6f75ddae1639"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUiP_jiK2C2_",
        "outputId": "4a585538-cc17-469a-fb10-9ca2cf87bafa"
      },
      "source": [
        "!pip install -r /content/drive/'My Drive'/'Colab Notebooks'/buddy/requirements.txt"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kobert-transformers==0.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: kogpt2-transformers==0.3.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 2)) (0.3.0)\n",
            "Requirement already satisfied: transformers==3.0.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 4)) (1.9.0+cu102)\n",
            "Requirement already satisfied: tokenizers==0.8.1rc1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 5)) (0.8.1rc1)\n",
            "Requirement already satisfied: kss in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 6)) (2.5.1)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (1.1.4)\n",
            "Requirement already satisfied: flask_restful in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 8)) (0.3.9)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (0.1.96)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 4)) (3.7.4.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (2.0.1)\n",
            "Requirement already satisfied: six>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from flask_restful->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 8)) (1.15.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from flask_restful->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 8)) (2018.9)\n",
            "Requirement already satisfied: aniso8601>=0.82 in /usr/local/lib/python3.7/dist-packages (from flask_restful->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 8)) (9.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9ONaA7l2Hmh"
      },
      "source": [
        "import sys\n",
        "sys.path.append('drive/My Drive/Colab Notebooks/')\n",
        "sys.path.append('drive/My Drive/Colab Notebooks/buddy')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4X8hDOC2JOy"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from transformers import AdamW\n",
        "from torch.utils.data import dataloader\n",
        "from buddy.dataloader.wellness import WellnessTextClassificationDataset\n",
        "from buddy.model.kobert import KoBERTforSequenceClassfication"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOvLUSCQ2KxO",
        "outputId": "ee36ffda-8827-481f-f518-17b00919ad3c"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1P6OOtM2LfD"
      },
      "source": [
        "def train(device, epoch, model, optimizer, train_loader, save_step, save_ckpt_path, train_step = 0):\n",
        "    losses = []\n",
        "    train_start_index = train_step+1 if train_step != 0 else 0\n",
        "    total_train_step = len(train_loader)\n",
        "    model.train()\n",
        "\n",
        "    with tqdm(total= total_train_step, desc=f\"Train({epoch})\") as pbar:\n",
        "        pbar.update(train_step)\n",
        "        for i, data in enumerate(train_loader, train_start_index):\n",
        "          \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(**data)\n",
        "\n",
        "            loss = outputs[0]\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix_str(f\"Loss: {loss.item():.3f} ({np.mean(losses):.3f})\")\n",
        "\n",
        "            if i >= total_train_step or i % save_step == 0:\n",
        "                torch.save({\n",
        "                    'epoch': epoch,  # 현재 학습 epoch\n",
        "                    'model_state_dict': model.state_dict(),  # 모델 저장\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),  # 옵티마이저 저장\n",
        "                    'loss': loss.item(),  # Loss 저장\n",
        "                    'train_step': i,  # 현재 진행한 학습\n",
        "                    'total_train_step': len(train_loader)  # 현재 epoch에 학습 할 총 train step\n",
        "                }, save_ckpt_path)\n",
        "\n",
        "    return np.mean(losses)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meIpH3HW2NOO"
      },
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdLIHijo2N38",
        "outputId": "9b17d597-f0b8-4a16-dbe0-11bb4a00173f"
      },
      "source": [
        "root_path = \"/content/drive/MyDrive/Colab Notebooks/buddy\"\n",
        "data_path = f\"{root_path}/data2/wellness_dialog_for_text_classification_train.txt\"\n",
        "checkpoint_path =f\"{root_path}/checkpoint\"\n",
        "save_ckpt_path = f\"{checkpoint_path}/kobert-wellness-text-classification-dataX-191.pth\"\n",
        "\n",
        "n_epoch = 50 #Num of Epoch\n",
        "batch_size = 4 #배치 사이즈 #Colab이 돌아가지 않아 4로 했으며, 증가시켜도 무방\n",
        "ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(ctx)\n",
        "save_step = 100 #학습 저장 주기\n",
        "learning_rate = 5e-6  #Learning Rate\n",
        "\n",
        "#WellnessTextClassificationDataset Data Loader\n",
        "dataset = WellnessTextClassificationDataset(file_path=data_path, device=device)\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = KoBERTforSequenceClassfication()\n",
        "model.to(device)\n",
        "\n",
        "#Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "      'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "\n",
        "pre_epoch, pre_loss, train_step = 0, 0, 0\n",
        "if os.path.isfile(save_ckpt_path):\n",
        "    checkpoint = torch.load(save_ckpt_path, map_location=device)\n",
        "    pre_epoch = checkpoint['epoch']\n",
        "    train_step =  checkpoint['train_step']\n",
        "    total_train_step =  checkpoint['total_train_step']\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    print(f\"load pretrain from: {save_ckpt_path}, epoch={pre_epoch}\")\n",
        "\n",
        "losses = []\n",
        "offset = pre_epoch\n",
        "for step in range(n_epoch):\n",
        "    epoch = step + offset\n",
        "    loss = train(device, epoch, model, optimizer, train_loader, save_step, save_ckpt_path, train_step)\n",
        "    losses.append(loss)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train(0): 100%|██████████| 1135/1135 [03:41<00:00,  5.13it/s, Loss: 4.734 (4.955)]\n",
            "Train(1): 100%|██████████| 1135/1135 [03:43<00:00,  5.08it/s, Loss: 5.315 (4.662)]\n",
            "Train(2): 100%|██████████| 1135/1135 [03:45<00:00,  5.04it/s, Loss: 4.709 (4.558)]\n",
            "Train(3): 100%|██████████| 1135/1135 [03:41<00:00,  5.12it/s, Loss: 5.734 (4.308)]\n",
            "Train(4): 100%|██████████| 1135/1135 [03:42<00:00,  5.09it/s, Loss: 3.312 (3.984)]\n",
            "Train(5): 100%|██████████| 1135/1135 [03:42<00:00,  5.11it/s, Loss: 3.112 (3.671)]\n",
            "Train(6): 100%|██████████| 1135/1135 [03:43<00:00,  5.09it/s, Loss: 2.640 (3.386)]\n",
            "Train(7): 100%|██████████| 1135/1135 [03:41<00:00,  5.13it/s, Loss: 0.682 (3.127)]\n",
            "Train(8): 100%|██████████| 1135/1135 [03:42<00:00,  5.09it/s, Loss: 0.712 (2.882)]\n",
            "Train(9): 100%|██████████| 1135/1135 [03:41<00:00,  5.12it/s, Loss: 2.795 (2.663)]\n",
            "Train(10): 100%|██████████| 1135/1135 [03:42<00:00,  5.11it/s, Loss: 4.713 (2.449)]\n",
            "Train(11): 100%|██████████| 1135/1135 [03:40<00:00,  5.14it/s, Loss: 2.078 (2.234)]\n",
            "Train(12): 100%|██████████| 1135/1135 [03:44<00:00,  5.06it/s, Loss: 1.480 (2.041)]\n",
            "Train(13): 100%|██████████| 1135/1135 [03:41<00:00,  5.13it/s, Loss: 0.877 (1.855)]\n",
            "Train(14): 100%|██████████| 1135/1135 [03:41<00:00,  5.13it/s, Loss: 0.575 (1.684)]\n",
            "Train(15): 100%|██████████| 1135/1135 [03:41<00:00,  5.13it/s, Loss: 3.233 (1.530)]\n",
            "Train(16): 100%|██████████| 1135/1135 [03:41<00:00,  5.13it/s, Loss: 2.368 (1.374)]\n",
            "Train(17): 100%|██████████| 1135/1135 [03:45<00:00,  5.04it/s, Loss: 2.533 (1.220)]\n",
            "Train(18): 100%|██████████| 1135/1135 [03:44<00:00,  5.06it/s, Loss: 1.852 (1.096)]\n",
            "Train(19): 100%|██████████| 1135/1135 [03:44<00:00,  5.05it/s, Loss: 1.662 (0.973)]\n",
            "Train(20): 100%|██████████| 1135/1135 [03:40<00:00,  5.14it/s, Loss: 1.122 (0.865)]\n",
            "Train(21): 100%|██████████| 1135/1135 [03:44<00:00,  5.05it/s, Loss: 1.169 (0.754)]\n",
            "Train(22): 100%|██████████| 1135/1135 [03:43<00:00,  5.08it/s, Loss: 1.038 (0.659)]\n",
            "Train(23): 100%|██████████| 1135/1135 [03:44<00:00,  5.05it/s, Loss: 0.507 (0.577)]\n",
            "Train(24): 100%|██████████| 1135/1135 [03:43<00:00,  5.07it/s, Loss: 0.076 (0.500)]\n",
            "Train(25): 100%|██████████| 1135/1135 [03:42<00:00,  5.09it/s, Loss: 0.124 (0.424)]\n",
            "Train(26): 100%|██████████| 1135/1135 [03:42<00:00,  5.09it/s, Loss: 0.128 (0.364)]\n",
            "Train(27): 100%|██████████| 1135/1135 [03:42<00:00,  5.10it/s, Loss: 0.210 (0.311)]\n",
            "Train(28): 100%|██████████| 1135/1135 [03:47<00:00,  5.00it/s, Loss: 0.525 (0.257)]\n",
            "Train(29): 100%|██████████| 1135/1135 [03:42<00:00,  5.09it/s, Loss: 0.131 (0.219)]\n",
            "Train(30): 100%|██████████| 1135/1135 [03:48<00:00,  4.98it/s, Loss: 0.011 (0.185)]\n",
            "Train(31): 100%|██████████| 1135/1135 [03:43<00:00,  5.08it/s, Loss: 0.003 (0.159)]\n",
            "Train(32): 100%|██████████| 1135/1135 [03:43<00:00,  5.07it/s, Loss: 0.394 (0.125)]\n",
            "Train(33): 100%|██████████| 1135/1135 [03:42<00:00,  5.10it/s, Loss: 0.010 (0.118)]\n",
            "Train(34): 100%|██████████| 1135/1135 [03:45<00:00,  5.04it/s, Loss: 0.136 (0.089)]\n",
            "Train(35): 100%|██████████| 1135/1135 [03:44<00:00,  5.05it/s, Loss: 0.018 (0.080)]\n",
            "Train(36): 100%|██████████| 1135/1135 [03:44<00:00,  5.06it/s, Loss: 0.083 (0.071)]\n",
            "Train(37): 100%|██████████| 1135/1135 [03:47<00:00,  4.99it/s, Loss: 0.018 (0.054)]\n",
            "Train(38): 100%|██████████| 1135/1135 [03:50<00:00,  4.92it/s, Loss: 0.017 (0.049)]\n",
            "Train(39): 100%|██████████| 1135/1135 [03:49<00:00,  4.94it/s, Loss: 0.014 (0.040)]\n",
            "Train(40): 100%|██████████| 1135/1135 [03:46<00:00,  5.02it/s, Loss: 0.016 (0.031)]\n",
            "Train(41): 100%|██████████| 1135/1135 [03:44<00:00,  5.06it/s, Loss: 0.058 (0.049)]\n",
            "Train(42): 100%|██████████| 1135/1135 [03:44<00:00,  5.05it/s, Loss: 0.010 (0.037)]\n",
            "Train(43): 100%|██████████| 1135/1135 [03:43<00:00,  5.08it/s, Loss: 0.029 (0.024)]\n",
            "Train(44): 100%|██████████| 1135/1135 [03:48<00:00,  4.96it/s, Loss: 0.011 (0.038)]\n",
            "Train(45): 100%|██████████| 1135/1135 [03:47<00:00,  4.99it/s, Loss: 0.005 (0.036)]\n",
            "Train(46): 100%|██████████| 1135/1135 [03:46<00:00,  5.01it/s, Loss: 0.007 (0.017)]\n",
            "Train(47): 100%|██████████| 1135/1135 [03:44<00:00,  5.05it/s, Loss: 0.011 (0.029)]\n",
            "Train(48): 100%|██████████| 1135/1135 [03:43<00:00,  5.08it/s, Loss: 0.014 (0.011)]\n",
            "Train(49): 100%|██████████| 1135/1135 [03:43<00:00,  5.07it/s, Loss: 0.003 (0.009)]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8zu_s1DZYF-3",
        "outputId": "4a5a35d9-d94b-4e9c-bb8d-46970885b362"
      },
      "source": [
        "\n",
        "# data\n",
        "data = {\n",
        "    \"loss\": losses\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "display(df)\n",
        "\n",
        "# graph\n",
        "plt.figure(figsize=[12, 4])\n",
        "plt.plot(losses, label=\"loss\")\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.955169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.661789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.558402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.308131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.984404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3.670903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3.386173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3.126829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2.881723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2.662740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2.448994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2.233940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2.041212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1.854700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1.684056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1.529550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1.374091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1.219666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1.096103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.973070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.864615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.754126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.658698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.576670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.499794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.424263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.363503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.311161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.257416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.219440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.185431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.159319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.124852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.117628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.089276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.079883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.070738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.054474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.048520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.039915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.031055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.048666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.037243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.024024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.038079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.036065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.017026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.029190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.011008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.008993</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        loss\n",
              "0   4.955169\n",
              "1   4.661789\n",
              "2   4.558402\n",
              "3   4.308131\n",
              "4   3.984404\n",
              "5   3.670903\n",
              "6   3.386173\n",
              "7   3.126829\n",
              "8   2.881723\n",
              "9   2.662740\n",
              "10  2.448994\n",
              "11  2.233940\n",
              "12  2.041212\n",
              "13  1.854700\n",
              "14  1.684056\n",
              "15  1.529550\n",
              "16  1.374091\n",
              "17  1.219666\n",
              "18  1.096103\n",
              "19  0.973070\n",
              "20  0.864615\n",
              "21  0.754126\n",
              "22  0.658698\n",
              "23  0.576670\n",
              "24  0.499794\n",
              "25  0.424263\n",
              "26  0.363503\n",
              "27  0.311161\n",
              "28  0.257416\n",
              "29  0.219440\n",
              "30  0.185431\n",
              "31  0.159319\n",
              "32  0.124852\n",
              "33  0.117628\n",
              "34  0.089276\n",
              "35  0.079883\n",
              "36  0.070738\n",
              "37  0.054474\n",
              "38  0.048520\n",
              "39  0.039915\n",
              "40  0.031055\n",
              "41  0.048666\n",
              "42  0.037243\n",
              "43  0.024024\n",
              "44  0.038079\n",
              "45  0.036065\n",
              "46  0.017026\n",
              "47  0.029190\n",
              "48  0.011008\n",
              "49  0.008993"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xKaN3xkMUIV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "e73b05f7-cbe0-44c6-b608-a5f813cfb528"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "from model.kobert import KoBERTforSequenceClassfication, kobert_input\n",
        "from kobert_transformers import get_tokenizer\n",
        "\n",
        "def load_wellness_answer():\n",
        "  root_path = \"/content/drive/MyDrive/Colab Notebooks/buddy\"\n",
        "  category_path = f\"{root_path}/data2/wellness_dialog_category.txt\"\n",
        "  answer_path = f\"{root_path}/data2/wellness_dialog_answer.txt\"\n",
        "\n",
        "  c_f = open(category_path,'r')\n",
        "  a_f = open(answer_path,'r')\n",
        "\n",
        "  category_lines = c_f.readlines()\n",
        "  answer_lines = a_f.readlines()\n",
        "\n",
        "  category = {}\n",
        "  answer = {}\n",
        "  for line_num, line_data in enumerate(category_lines):\n",
        "    data = line_data.split('    ')\n",
        "    category[data[1][:-1]]=data[0]\n",
        "  \n",
        "  for line_num, line_data in enumerate(answer_lines):\n",
        "    data = line_data.split('    ')\n",
        "    keys = answer.keys()\n",
        "    if(data[0] in keys):\n",
        "      answer[data[0]] += [data[1][:-1]]\n",
        "    else:\n",
        "      answer[data[0]] = [data[1][:-1]]\n",
        "\n",
        "  return category, answer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  root_path = \"/content/drive/MyDrive/Colab Notebooks/buddy\"\n",
        "  checkpoint_path =f\"{root_path}/checkpoint\"\n",
        "  save_ckpt_path = f\"{checkpoint_path}/kobert-wellness-text-classification-dataX-191.pth\"\n",
        "\n",
        "  #답변과 카테고리 불러오기\n",
        "  category, answer = load_wellness_answer()\n",
        "\n",
        "  ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  device = torch.device(ctx)\n",
        "\n",
        "  # 저장한 Checkpoint 불러오기\n",
        "  checkpoint = torch.load(save_ckpt_path, map_location=device)\n",
        "\n",
        "  model = KoBERTforSequenceClassfication()\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "  model.to(ctx)\n",
        "  model.eval()\n",
        "\n",
        "  tokenizer = get_tokenizer()\n",
        "\n",
        "  while 1:\n",
        "    sent = input('\\nQuestion: ') # '요즘 기분이 우울한 느낌이에요'\n",
        "    data = kobert_input(tokenizer, sent, device, 512)\n",
        "\n",
        "    if '종료' in sent:\n",
        "      break\n",
        "\n",
        "    output = model(**data)\n",
        "\n",
        "    logit = output[0]\n",
        "    softmax_logit = torch.softmax(logit,dim=-1)\n",
        "    softmax_logit = softmax_logit.squeeze()\n",
        "\n",
        "    max_index = torch.argmax(softmax_logit).item()\n",
        "    max_index_value = softmax_logit[torch.argmax(softmax_logit)].item()\n",
        "\n",
        "    answer_list = answer[category[str(max_index)]]\n",
        "    answer_len= len(answer_list)-1\n",
        "    answer_index = random.randint(0,answer_len)\n",
        "    print(f'Answer: {answer_list[answer_index]}, index: {max_index}, softmax_value: {max_index_value}')\n",
        "    print('-'*50)\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-ef4c46252a1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nQuestion: '\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# '요즘 기분이 우울한 느낌이에요'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkobert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW0_62qtJD3q"
      },
      "source": [
        "평가함수\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6l63ZOTJDs5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abad0932-f31f-4232-d1c3-370ef7c0d6ee"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "  AdamW,\n",
        "  ElectraConfig,\n",
        "  ElectraTokenizer\n",
        ")\n",
        "from torch.utils.data import dataloader\n",
        "from dataloader.wellness import WellnessTextClassificationDataset\n",
        "#from model.koelectra import koElectraForSequenceClassification\n",
        "from model.kobert import KoBERTforSequenceClassfication\n",
        "from kobert_transformers import get_tokenizer\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "MODEL_CLASSES ={\n",
        "  #\"koelectra\": (ElectraConfig, koElectraForSequenceClassification, ElectraTokenizer),\n",
        "  \"kobert\": (KoBERTforSequenceClassfication)\n",
        "}\n",
        "CHECK_POINT ={\n",
        "  #\"koelectra\": \"/content/drive/MyDrive/Colab Notebooks/buddy/checkpoint/koelectra-wellnesee-text-classification.pth\",\n",
        "  \"kobert\": \"/content/drive/MyDrive/Colab Notebooks/buddy/checkpoint/kobert-wellness-text-classification-dataX-191.pth\"\n",
        "}\n",
        "\n",
        "def get_model_and_tokenizer(model_name, device):\n",
        "  save_ckpt_path = CHECK_POINT[model_name]\n",
        "\n",
        "#  if model_name== \"koelectra\":\n",
        "#    model_name_or_path = \"monologg/koelectra-base-discriminator\"\n",
        "\n",
        "#    tokenizer = ElectraTokenizer.from_pretrained(model_name_or_path)\n",
        "#    electra_config = ElectraConfig.from_pretrained(model_name_or_path)\n",
        "#    model = koElectraForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path,\n",
        "#                                                               config=electra_config,\n",
        "#                                                               num_labels=359)\n",
        "  if model_name =='kobert':\n",
        "    tokenizer = get_tokenizer()\n",
        "    model = KoBERTforSequenceClassfication()\n",
        "\n",
        "  if os.path.isfile(save_ckpt_path):\n",
        "      checkpoint = torch.load(save_ckpt_path, map_location=device)\n",
        "      pre_epoch = checkpoint['epoch']\n",
        "      # pre_loss = checkpoint['loss']\n",
        "      model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "      print(f\"load pretrain from: {save_ckpt_path}, epoch={pre_epoch}\")\n",
        "\n",
        "  return model, tokenizer\n",
        "\n",
        "def get_model_input(data):\n",
        "  if model_name =='kobert':\n",
        "    return data\n",
        "#  elif model_name== \"koelectra\":\n",
        "#    return {'input_ids': data['input_ids'],\n",
        "#              'attention_mask': data['attention_mask'],\n",
        "#              'labels': data['labels']\n",
        "#              }\n",
        "\n",
        "def evaluate(model_name, device, batch_size, data_path):\n",
        "\n",
        "  model, tokenizer = get_model_and_tokenizer(model_name, device)\n",
        "  model.to(device)\n",
        "\n",
        "  # WellnessTextClassificationDataset 데이터 로더\n",
        "  eval_dataset = WellnessTextClassificationDataset(file_path=data_path,device=device, tokenizer=tokenizer)\n",
        "  eval_dataloader = torch.utils.data.DataLoader(eval_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  logger.info(\"***** Running evaluation on %s dataset *****\")\n",
        "  logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "  logger.info(\"  Batch size = %d\", batch_size)\n",
        "\n",
        "  loss = 0\n",
        "  acc = 0\n",
        "\n",
        "\n",
        "  # model.eval()\n",
        "  for data in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "    with torch.no_grad():\n",
        "      inputs = get_model_input(data)\n",
        "      outputs = model(**inputs)\n",
        "      loss += outputs[0]\n",
        "      logit = outputs[1]\n",
        "      acc += (logit.argmax(1)==inputs['labels']).sum().item()\n",
        "\n",
        "  return loss / len(eval_dataset), acc / len(eval_dataset)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  #root_path = \"/content/drive/MyDrive/Colab Notebooks/buddy/data\"\n",
        "  root_path = \"/content/drive/MyDrive/Colab Notebooks/buddy\"\n",
        "  data_path = f\"{root_path}/data/wellness_dialog_for_text_classification_test.txt\"\n",
        "  checkpoint_path = f\"{root_path}/checkpoint\"\n",
        "  save_ckpt_path = f\"{checkpoint_path}/kobert-wellness-text-classification-dataX-191.pth\"\n",
        "  #model_name_or_path = \"monologg/koelectra-base-discriminator\"\n",
        "\n",
        "  n_epoch = 50  # Num of Epoch\n",
        "  batch_size = 16  # 배치 사이즈\n",
        "  ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  device = torch.device(ctx)\n",
        "  model_names=[\"kobert\"]\n",
        "  for model_name in model_names:\n",
        "    eval_loss, eval_acc = evaluate(model_name, device, batch_size, data_path)\n",
        "    print(f'\\tLoss: {eval_loss:.4f}(valid)\\t|\\tAcc: {eval_acc * 100:.1f}%(valid)')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load pretrain from: /content/drive/MyDrive/Colab Notebooks/buddy/checkpoint/kobert-wellness-text-classification-dataX-191.pth, epoch=49\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 33/33 [00:05<00:00,  6.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tLoss: 0.8970(valid)\t|\tAcc: 0.2%(valid)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73KXoMIagVS8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}