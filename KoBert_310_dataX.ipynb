{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "KoBert_310_dataX",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP3HyedfPl5epkTs0p/roJ7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_bwTQhc-EwG",
        "outputId": "bec43945-c1cb-4d71-fcee-6731865bf7d5"
      },
      "source": [
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-dugovm50\n",
            "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-dugovm50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD2qEE_4-KW8",
        "outputId": "3ac7bbef-4c70-4a59-e695-7dc1f58ee22d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUiP_jiK2C2_",
        "outputId": "0ef197ad-1c8f-457a-8030-880878f86be4"
      },
      "source": [
        "!pip install -r /content/drive/'My Drive'/'Colab Notebooks'/buddy/requirements.txt"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kobert-transformers==0.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: kogpt2-transformers==0.3.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 2)) (0.3.0)\n",
            "Requirement already satisfied: transformers==3.0.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 4)) (1.9.0+cu102)\n",
            "Requirement already satisfied: tokenizers==0.8.1rc1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 5)) (0.8.1rc1)\n",
            "Requirement already satisfied: kss in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 6)) (2.6.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (1.1.4)\n",
            "Requirement already satisfied: flask_restful in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 8)) (0.3.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (0.0.45)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (0.1.96)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (4.62.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 4)) (3.7.4.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 7)) (2.0.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from flask_restful->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 8)) (2018.9)\n",
            "Requirement already satisfied: aniso8601>=0.82 in /usr/local/lib/python3.7/dist-packages (from flask_restful->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 8)) (9.0.1)\n",
            "Requirement already satisfied: six>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from flask_restful->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 8)) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (2021.5.30)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2->-r /content/drive/My Drive/Colab Notebooks/buddy/requirements.txt (line 3)) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9ONaA7l2Hmh"
      },
      "source": [
        "import sys\n",
        "sys.path.append('drive/My Drive/Colab Notebooks/')\n",
        "sys.path.append('drive/My Drive/Colab Notebooks/buddy')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4X8hDOC2JOy"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from transformers import AdamW\n",
        "from torch.utils.data import dataloader\n",
        "from buddy.dataloader.wellness import WellnessTextClassificationDataset\n",
        "from buddy.model.kobert import KoBERTforSequenceClassfication"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOvLUSCQ2KxO",
        "outputId": "d6429f29-a18b-4ddb-c53f-6338b17ff707"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1P6OOtM2LfD"
      },
      "source": [
        "def train(device, epoch, model, optimizer, train_loader, save_step, save_ckpt_path, train_step = 0):\n",
        "    losses = []\n",
        "    train_start_index = train_step+1 if train_step != 0 else 0\n",
        "    total_train_step = len(train_loader)\n",
        "    model.train()\n",
        "\n",
        "    with tqdm(total= total_train_step, desc=f\"Train({epoch})\") as pbar:\n",
        "        pbar.update(train_step)\n",
        "        for i, data in enumerate(train_loader, train_start_index):\n",
        "          \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(**data)\n",
        "\n",
        "            loss = outputs[0]\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix_str(f\"Loss: {loss.item():.3f} ({np.mean(losses):.3f})\")\n",
        "\n",
        "            if i >= total_train_step or i % save_step == 0:\n",
        "                torch.save({\n",
        "                    'epoch': epoch,  # 현재 학습 epoch\n",
        "                    'model_state_dict': model.state_dict(),  # 모델 저장\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),  # 옵티마이저 저장\n",
        "                    'loss': loss.item(),  # Loss 저장\n",
        "                    'train_step': i,  # 현재 진행한 학습\n",
        "                    'total_train_step': len(train_loader)  # 현재 epoch에 학습 할 총 train step\n",
        "                }, save_ckpt_path)\n",
        "\n",
        "    return np.mean(losses)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meIpH3HW2NOO"
      },
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdLIHijo2N38",
        "outputId": "a02d9ab1-be76-4a9f-ea67-85cdc8cb68fa"
      },
      "source": [
        "root_path = \"/content/drive/MyDrive/Colab Notebooks/buddy\"\n",
        "data_path = f\"{root_path}/data2/wellness_dialog_for_text_classification_train.txt\"\n",
        "checkpoint_path =f\"{root_path}/checkpoint\"\n",
        "save_ckpt_path = f\"{checkpoint_path}/kobert-wellness-text-classification-dataX-310.pth\"\n",
        "\n",
        "n_epoch = 50 #Num of Epoch\n",
        "batch_size = 4 #배치 사이즈 #Colab이 돌아가지 않아 4로 했으며, 증가시켜도 무방\n",
        "ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(ctx)\n",
        "save_step = 100 #학습 저장 주기\n",
        "learning_rate = 5e-6  #Learning Rate\n",
        "\n",
        "#WellnessTextClassificationDataset Data Loader\n",
        "dataset = WellnessTextClassificationDataset(file_path=data_path, device=device)\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = KoBERTforSequenceClassfication()\n",
        "model.to(device)\n",
        "\n",
        "#Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "      'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "\n",
        "pre_epoch, pre_loss, train_step = 0, 0, 0\n",
        "if os.path.isfile(save_ckpt_path):\n",
        "    checkpoint = torch.load(save_ckpt_path, map_location=device)\n",
        "    pre_epoch = checkpoint['epoch']\n",
        "    train_step =  checkpoint['train_step']\n",
        "    total_train_step =  checkpoint['total_train_step']\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    print(f\"load pretrain from: {save_ckpt_path}, epoch={pre_epoch}\")\n",
        "\n",
        "losses = []\n",
        "offset = pre_epoch\n",
        "for step in range(n_epoch):\n",
        "    epoch = step + offset\n",
        "    loss = train(device, epoch, model, optimizer, train_loader, save_step, save_ckpt_path, train_step)\n",
        "    losses.append(loss)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train(0): 100%|██████████| 1189/1189 [03:38<00:00,  5.43it/s, Loss: 4.998 (5.512)]\n",
            "Train(1): 100%|██████████| 1189/1189 [03:41<00:00,  5.37it/s, Loss: 5.834 (5.351)]\n",
            "Train(2): 100%|██████████| 1189/1189 [03:37<00:00,  5.45it/s, Loss: 5.122 (5.242)]\n",
            "Train(3): 100%|██████████| 1189/1189 [03:44<00:00,  5.30it/s, Loss: 5.757 (5.036)]\n",
            "Train(4): 100%|██████████| 1189/1189 [03:47<00:00,  5.22it/s, Loss: 5.632 (4.744)]\n",
            "Train(5): 100%|██████████| 1189/1189 [04:04<00:00,  4.87it/s, Loss: 4.418 (4.454)]\n",
            "Train(6): 100%|██████████| 1189/1189 [03:40<00:00,  5.40it/s, Loss: 3.001 (4.188)]\n",
            "Train(7): 100%|██████████| 1189/1189 [03:44<00:00,  5.31it/s, Loss: 3.165 (3.947)]\n",
            "Train(8): 100%|██████████| 1189/1189 [03:46<00:00,  5.26it/s, Loss: 4.173 (3.707)]\n",
            "Train(9): 100%|██████████| 1189/1189 [03:48<00:00,  5.20it/s, Loss: 3.557 (3.475)]\n",
            "Train(10): 100%|██████████| 1189/1189 [04:04<00:00,  4.87it/s, Loss: 1.195 (3.250)]\n",
            "Train(11): 100%|██████████| 1189/1189 [03:41<00:00,  5.36it/s, Loss: 3.734 (3.045)]\n",
            "Train(12): 100%|██████████| 1189/1189 [03:41<00:00,  5.37it/s, Loss: 2.631 (2.834)]\n",
            "Train(13): 100%|██████████| 1189/1189 [03:43<00:00,  5.33it/s, Loss: 3.864 (2.657)]\n",
            "Train(14): 100%|██████████| 1189/1189 [03:42<00:00,  5.35it/s, Loss: 3.033 (2.456)]\n",
            "Train(15): 100%|██████████| 1189/1189 [03:44<00:00,  5.30it/s, Loss: 1.817 (2.284)]\n",
            "Train(16): 100%|██████████| 1189/1189 [03:46<00:00,  5.26it/s, Loss: 0.468 (2.108)]\n",
            "Train(17): 100%|██████████| 1189/1189 [03:46<00:00,  5.25it/s, Loss: 2.079 (1.928)]\n",
            "Train(18): 100%|██████████| 1189/1189 [03:41<00:00,  5.36it/s, Loss: 0.928 (1.764)]\n",
            "Train(19): 100%|██████████| 1189/1189 [03:41<00:00,  5.38it/s, Loss: 1.075 (1.622)]\n",
            "Train(20): 100%|██████████| 1189/1189 [03:41<00:00,  5.37it/s, Loss: 1.547 (1.470)]\n",
            "Train(21): 100%|██████████| 1189/1189 [03:46<00:00,  5.25it/s, Loss: 1.017 (1.325)]\n",
            "Train(22): 100%|██████████| 1189/1189 [03:44<00:00,  5.29it/s, Loss: 1.253 (1.195)]\n",
            "Train(23): 100%|██████████| 1189/1189 [03:42<00:00,  5.34it/s, Loss: 1.741 (1.076)]\n",
            "Train(24): 100%|██████████| 1189/1189 [03:43<00:00,  5.32it/s, Loss: 0.208 (0.958)]\n",
            "Train(25): 100%|██████████| 1189/1189 [03:45<00:00,  5.27it/s, Loss: 0.555 (0.857)]\n",
            "Train(26): 100%|██████████| 1189/1189 [03:43<00:00,  5.33it/s, Loss: 0.910 (0.755)]\n",
            "Train(27): 100%|██████████| 1189/1189 [03:42<00:00,  5.35it/s, Loss: 0.320 (0.666)]\n",
            "Train(28): 100%|██████████| 1189/1189 [03:45<00:00,  5.28it/s, Loss: 1.105 (0.580)]\n",
            "Train(29): 100%|██████████| 1189/1189 [03:44<00:00,  5.29it/s, Loss: 0.177 (0.505)]\n",
            "Train(30): 100%|██████████| 1189/1189 [03:41<00:00,  5.38it/s, Loss: 0.166 (0.432)]\n",
            "Train(31): 100%|██████████| 1189/1189 [03:40<00:00,  5.40it/s, Loss: 0.480 (0.369)]\n",
            "Train(32): 100%|██████████| 1189/1189 [03:38<00:00,  5.44it/s, Loss: 0.299 (0.317)]\n",
            "Train(33): 100%|██████████| 1189/1189 [03:40<00:00,  5.40it/s, Loss: 0.315 (0.261)]\n",
            "Train(34): 100%|██████████| 1189/1189 [03:41<00:00,  5.36it/s, Loss: 0.259 (0.228)]\n",
            "Train(35): 100%|██████████| 1189/1189 [03:42<00:00,  5.34it/s, Loss: 0.131 (0.189)]\n",
            "Train(36): 100%|██████████| 1189/1189 [03:42<00:00,  5.34it/s, Loss: 0.044 (0.151)]\n",
            "Train(37): 100%|██████████| 1189/1189 [03:54<00:00,  5.08it/s, Loss: 0.040 (0.124)]\n",
            "Train(38): 100%|██████████| 1189/1189 [03:34<00:00,  5.55it/s, Loss: 0.260 (0.106)]\n",
            "Train(39): 100%|██████████| 1189/1189 [03:39<00:00,  5.43it/s, Loss: 0.069 (0.085)]\n",
            "Train(40): 100%|██████████| 1189/1189 [03:44<00:00,  5.30it/s, Loss: 0.039 (0.073)]\n",
            "Train(41): 100%|██████████| 1189/1189 [03:39<00:00,  5.42it/s, Loss: 0.094 (0.061)]\n",
            "Train(42): 100%|██████████| 1189/1189 [03:40<00:00,  5.39it/s, Loss: 0.040 (0.058)]\n",
            "Train(43): 100%|██████████| 1189/1189 [03:41<00:00,  5.36it/s, Loss: 0.019 (0.039)]\n",
            "Train(44): 100%|██████████| 1189/1189 [03:40<00:00,  5.38it/s, Loss: 0.032 (0.046)]\n",
            "Train(45): 100%|██████████| 1189/1189 [03:41<00:00,  5.38it/s, Loss: 0.016 (0.030)]\n",
            "Train(46): 100%|██████████| 1189/1189 [03:42<00:00,  5.35it/s, Loss: 0.017 (0.022)]\n",
            "Train(47): 100%|██████████| 1189/1189 [03:40<00:00,  5.40it/s, Loss: 0.022 (0.018)]\n",
            "Train(48): 100%|██████████| 1189/1189 [03:39<00:00,  5.43it/s, Loss: 0.010 (0.034)]\n",
            "Train(49): 100%|██████████| 1189/1189 [03:42<00:00,  5.34it/s, Loss: 0.608 (0.021)]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zu_s1DZYF-3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1d879284-bedc-494c-fdcf-21b75fa6665c"
      },
      "source": [
        "\n",
        "# data\n",
        "data = {\n",
        "    \"loss\": losses\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "display(df)\n",
        "\n",
        "# graph\n",
        "plt.figure(figsize=[12, 4])\n",
        "plt.plot(losses, label=\"loss\")\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.512088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.350970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.242226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.036379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.744312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4.454038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4.188474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3.947158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3.707173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3.475132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>3.250472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>3.045278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2.834191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2.656562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2.455761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2.284334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2.107922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1.927727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1.763615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1.622008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1.469782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1.325366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1.194576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1.075907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.958040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.857104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.754605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.666078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.580422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.505338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.432401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.368624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.316555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.261300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.227722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.189396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.151132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.124471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.106439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.084686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.073438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.061142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.057646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.039032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.045895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.029567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.022123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.018481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.033913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.021145</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        loss\n",
              "0   5.512088\n",
              "1   5.350970\n",
              "2   5.242226\n",
              "3   5.036379\n",
              "4   4.744312\n",
              "5   4.454038\n",
              "6   4.188474\n",
              "7   3.947158\n",
              "8   3.707173\n",
              "9   3.475132\n",
              "10  3.250472\n",
              "11  3.045278\n",
              "12  2.834191\n",
              "13  2.656562\n",
              "14  2.455761\n",
              "15  2.284334\n",
              "16  2.107922\n",
              "17  1.927727\n",
              "18  1.763615\n",
              "19  1.622008\n",
              "20  1.469782\n",
              "21  1.325366\n",
              "22  1.194576\n",
              "23  1.075907\n",
              "24  0.958040\n",
              "25  0.857104\n",
              "26  0.754605\n",
              "27  0.666078\n",
              "28  0.580422\n",
              "29  0.505338\n",
              "30  0.432401\n",
              "31  0.368624\n",
              "32  0.316555\n",
              "33  0.261300\n",
              "34  0.227722\n",
              "35  0.189396\n",
              "36  0.151132\n",
              "37  0.124471\n",
              "38  0.106439\n",
              "39  0.084686\n",
              "40  0.073438\n",
              "41  0.061142\n",
              "42  0.057646\n",
              "43  0.039032\n",
              "44  0.045895\n",
              "45  0.029567\n",
              "46  0.022123\n",
              "47  0.018481\n",
              "48  0.033913\n",
              "49  0.021145"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xKaN3xkMUIV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "f214f517-8aef-487a-b45c-b7d80ee19385"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "from model.kobert import KoBERTforSequenceClassfication, kobert_input\n",
        "from kobert_transformers import get_tokenizer\n",
        "\n",
        "def load_wellness_answer():\n",
        "  root_path = \"/content/drive/MyDrive/Colab Notebooks/buddy\"\n",
        "  category_path = f\"{root_path}/data2/wellness_dialog_category.txt\"\n",
        "  answer_path = f\"{root_path}/data2/wellness_dialog_answer.txt\"\n",
        "\n",
        "  c_f = open(category_path,'r')\n",
        "  a_f = open(answer_path,'r')\n",
        "\n",
        "  category_lines = c_f.readlines()\n",
        "  answer_lines = a_f.readlines()\n",
        "\n",
        "  category = {}\n",
        "  answer = {}\n",
        "  for line_num, line_data in enumerate(category_lines):\n",
        "    data = line_data.split('    ')\n",
        "    category[data[1][:-1]]=data[0]\n",
        "  \n",
        "  for line_num, line_data in enumerate(answer_lines):\n",
        "    data = line_data.split('    ')\n",
        "    keys = answer.keys()\n",
        "    if(data[0] in keys):\n",
        "      answer[data[0]] += [data[1][:-1]]\n",
        "    else:\n",
        "      answer[data[0]] = [data[1][:-1]]\n",
        "\n",
        "  return category, answer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  root_path = \"/content/drive/MyDrive/Colab Notebooks/buddy\"\n",
        "  checkpoint_path =f\"{root_path}/checkpoint\"\n",
        "  save_ckpt_path = f\"{checkpoint_path}/kobert-wellness-text-classification-dataX-310.pth\"\n",
        "\n",
        "  #답변과 카테고리 불러오기\n",
        "  category, answer = load_wellness_answer()\n",
        "\n",
        "  ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  device = torch.device(ctx)\n",
        "\n",
        "  # 저장한 Checkpoint 불러오기\n",
        "  checkpoint = torch.load(save_ckpt_path, map_location=device)\n",
        "\n",
        "  model = KoBERTforSequenceClassfication()\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "  model.to(ctx)\n",
        "  model.eval()\n",
        "\n",
        "  tokenizer = get_tokenizer()\n",
        "\n",
        "  while 1:\n",
        "    sent = input('\\nQuestion: ') # '요즘 기분이 우울한 느낌이에요'\n",
        "    data = kobert_input(tokenizer, sent, device, 512)\n",
        "\n",
        "    if '종료' in sent:\n",
        "      break\n",
        "\n",
        "    output = model(**data)\n",
        "\n",
        "    logit = output[0]\n",
        "    softmax_logit = torch.softmax(logit,dim=-1)\n",
        "    softmax_logit = softmax_logit.squeeze()\n",
        "\n",
        "    max_index = torch.argmax(softmax_logit).item()\n",
        "    max_index_value = softmax_logit[torch.argmax(softmax_logit)].item()\n",
        "\n",
        "    answer_list = answer[category[str(max_index)]]\n",
        "    answer_len= len(answer_list)-1\n",
        "    answer_index = random.randint(0,answer_len)\n",
        "    print(f'Answer: {answer_list[answer_index]}, index: {max_index}, softmax_value: {max_index_value}')\n",
        "    print('-'*50)\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    618\u001b[0m         \"\"\"\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-ed2a5c104589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nQuestion: '\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# '요즘 기분이 우울한 느낌이에요'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkobert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW0_62qtJD3q"
      },
      "source": [
        "평가함수\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6l63ZOTJDs5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a2de0f0-04f1-46ac-cd1a-393473f58b00"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "  AdamW,\n",
        "  ElectraConfig,\n",
        "  ElectraTokenizer\n",
        ")\n",
        "from torch.utils.data import dataloader\n",
        "from dataloader.wellness import WellnessTextClassificationDataset\n",
        "#from model.koelectra import koElectraForSequenceClassification\n",
        "from model.kobert import KoBERTforSequenceClassfication\n",
        "from kobert_transformers import get_tokenizer\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "MODEL_CLASSES ={\n",
        "  #\"koelectra\": (ElectraConfig, koElectraForSequenceClassification, ElectraTokenizer),\n",
        "  \"kobert\": (KoBERTforSequenceClassfication)\n",
        "}\n",
        "CHECK_POINT ={\n",
        "  #\"koelectra\": \"/content/drive/MyDrive/Colab Notebooks/buddy/checkpoint/koelectra-wellnesee-text-classification.pth\",\n",
        "  \"kobert\": \"/content/drive/MyDrive/Colab Notebooks/buddy/checkpoint/kobert-wellness-text-classification-dataX-310.pth\"\n",
        "}\n",
        "\n",
        "def get_model_and_tokenizer(model_name, device):\n",
        "  save_ckpt_path = CHECK_POINT[model_name]\n",
        "\n",
        "#  if model_name== \"koelectra\":\n",
        "#    model_name_or_path = \"monologg/koelectra-base-discriminator\"\n",
        "\n",
        "#    tokenizer = ElectraTokenizer.from_pretrained(model_name_or_path)\n",
        "#    electra_config = ElectraConfig.from_pretrained(model_name_or_path)\n",
        "#    model = koElectraForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path,\n",
        "#                                                               config=electra_config,\n",
        "#                                                               num_labels=359)\n",
        "  if model_name =='kobert':\n",
        "    tokenizer = get_tokenizer()\n",
        "    model = KoBERTforSequenceClassfication()\n",
        "\n",
        "  if os.path.isfile(save_ckpt_path):\n",
        "      checkpoint = torch.load(save_ckpt_path, map_location=device)\n",
        "      pre_epoch = checkpoint['epoch']\n",
        "      # pre_loss = checkpoint['loss']\n",
        "      model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "      print(f\"load pretrain from: {save_ckpt_path}, epoch={pre_epoch}\")\n",
        "\n",
        "  return model, tokenizer\n",
        "\n",
        "def get_model_input(data):\n",
        "  if model_name =='kobert':\n",
        "    return data\n",
        "#  elif model_name== \"koelectra\":\n",
        "#    return {'input_ids': data['input_ids'],\n",
        "#              'attention_mask': data['attention_mask'],\n",
        "#              'labels': data['labels']\n",
        "#              }\n",
        "\n",
        "def evaluate(model_name, device, batch_size, data_path):\n",
        "\n",
        "  model, tokenizer = get_model_and_tokenizer(model_name, device)\n",
        "  model.to(device)\n",
        "\n",
        "  # WellnessTextClassificationDataset 데이터 로더\n",
        "  eval_dataset = WellnessTextClassificationDataset(file_path=data_path,device=device, tokenizer=tokenizer)\n",
        "  eval_dataloader = torch.utils.data.DataLoader(eval_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  logger.info(\"***** Running evaluation on %s dataset *****\")\n",
        "  logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "  logger.info(\"  Batch size = %d\", batch_size)\n",
        "\n",
        "  loss = 0\n",
        "  acc = 0\n",
        "\n",
        "\n",
        "  # model.eval()\n",
        "  for data in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "    with torch.no_grad():\n",
        "      inputs = get_model_input(data)\n",
        "      outputs = model(**inputs)\n",
        "      loss += outputs[0]\n",
        "      logit = outputs[1]\n",
        "      acc += (logit.argmax(1)==inputs['labels']).sum().item()\n",
        "\n",
        "  return loss / len(eval_dataset), acc / len(eval_dataset)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  #root_path = \"/content/drive/MyDrive/Colab Notebooks/buddy/data\"\n",
        "  root_path = \"/content/drive/MyDrive/Colab Notebooks/buddy\"\n",
        "  data_path = f\"{root_path}/data2/wellness_dialog_for_text_classification_test.txt\"\n",
        "  checkpoint_path = f\"{root_path}/checkpoint\"\n",
        "  save_ckpt_path = f\"{checkpoint_path}/kobert-wellness-text-classification-dataX-310.pth\"\n",
        "  #model_name_or_path = \"monologg/koelectra-base-discriminator\"\n",
        "\n",
        "  n_epoch = 50  # Num of Epoch\n",
        "  batch_size = 16  # 배치 사이즈\n",
        "  ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  device = torch.device(ctx)\n",
        "  model_names=[\"kobert\"]\n",
        "  for model_name in model_names:\n",
        "    eval_loss, eval_acc = evaluate(model_name, device, batch_size, data_path)\n",
        "    print(f'\\tLoss: {eval_loss:.4f}(valid)\\t|\\tAcc: {eval_acc * 100:.1f}%(valid)')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load pretrain from: /content/drive/MyDrive/Colab Notebooks/buddy/checkpoint/kobert-wellness-text-classification-dataX-310.pth, epoch=49\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 29/29 [00:04<00:00,  6.33it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tLoss: 0.2383(valid)\t|\tAcc: 37.6%(valid)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73KXoMIagVS8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}